\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{url}
\usepackage{graphicx}

\title{CSCI 4535/5535 - Natural Language Processing Project Report: Generating Constrained SVG with Qwen2.0}
\author{Henry Fordjour Ansah\\
        University of New Orleans\\
        \texttt{hfansah@uno.edu}}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
This report details a project focused on generating SVG code from natural language descriptions for the Kaggle competition "Drawing with LLMs." The project utilizes a Qwen2.0 reasoning model (either 7B or 16B) and employs a two-stage approach. First, the model undergoes supervised fine-tuning on the deepseek-svg-dataset. Second, to ensure the generated SVG adheres to specific competition constraints, reinforcement learning using the Group Relative Policy Optimization (GRPO) algorithm is applied. This project aims to develop a robust system capable of translating natural language into valid and constrained SVG code.
\end{abstract}

\section{Introduction}
The task of translating natural language into vector graphics holds significant potential for creative expression and programmatic content generation. This project addresses this challenge within the context of the Kaggle competition "Drawing with LLMs," which requires participants to generate SVG code from textual descriptions. The core approach involves leveraging the reasoning capabilities of a large language model, specifically the Qwen2.0 (7B or 16B) model, and refining its output through a combination of supervised fine-tuning and reinforcement learning. This methodology aims to produce SVG code that is not only semantically relevant to the input prompt but also strictly adheres to the competition's constraints on size, allowed elements and attributes, and the exclusion of rasterized data or external sources. This project builds upon recent advancements in large language models and their application to code generation and creative tasks. The expected outcome is a model capable of generating high-quality, constrained SVG code from natural language prompts, demonstrating the effectiveness of the proposed two-stage training process.

\section{Problem Definition and Algorithm}
\subsection{Task Definition}
The problem addressed in this project is the generation of valid and constrained Scalable Vector Graphics (SVG) code from natural language descriptions. The input to the system is a textual prompt describing an image or graphic. The output is a string of SVG code representing that image. This task is crucial for the "Drawing with LLMs" Kaggle competition, where submissions are evaluated based on the quality and constraint compliance of the generated SVGs. The constraints are: 1) No SVG may be more than 10,000 bytes long. 2) Each SVG may only include elements and attributes from an allowlist, excluding CSS style elements. 3) No SVG may include any rasterized image data or data from external sources. Additionally, the evaluation system requires that an SVG is returned within 5 minutes, and all SVGs are generated in under 9 hours. This problem is interesting due to the complexity of mapping natural language concepts to precise SVG syntax and the added challenge of enforcing strict output constraints.

\subsection{Algorithm Definition}
The proposed algorithm consists of two main stages: supervised fine-tuning and reinforcement learning.

\textbf{Stage 1: Supervised Fine-tuning}
In the first stage, a Qwen2.0 model (either the 7B or 16B variant) is fine-tuned using supervised learning on the deepseek-svg-dataset. This dataset contains a collection of natural language prompts, corresponding reasoning traces or chain of thought, and the ground truth SVG code. The fine-tuning process aims to teach the Qwen2.0 model the relationship between natural language instructions and the structure and syntax of SVG code. By learning from the reasoning traces, the model can potentially improve its ability to generate complex and coherent SVG structures. The training process involves feeding the model the prompt and reasoning trace as input and asking it to predict the target SVG code. Standard supervised learning techniques, such as backpropagation and gradient descent, are used to update the model's weights to minimize the difference between the predicted and the ground truth SVG code.

\textbf{Stage 2: Reinforcement Learning for Constraint Enforcement}
The second stage employs reinforcement learning to ensure that the SVG code generated by the fine-tuned model adheres to the competition's constraints. The Group Relative Policy Optimization (GRPO) algorithm is used for this purpose. In this stage, the model acts as an agent that generates SVG code based on a given prompt. The environment evaluates the generated SVG against the specified constraints and provides a reward signal. A high positive reward is given when the generated SVG satisfies all the constraints (size limit, allowlist of elements and attributes, no raster or external data). Conversely, negative or zero rewards are given for violations. The GRPO algorithm then updates the model's policy (i.e., the way it generates SVG code) to maximize the expected reward. By iteratively generating SVG code, receiving feedback, and updating its policy, the model learns to produce outputs that are more likely to comply with the given constraints. The constraints that are enforced during this stage are:
\begin{itemize}
    \item No SVG may more than 10,000 bytes long.
    \item Each SVG may only include elements and attributes from an allowlist. Note that CSS style elements are not allowed.
    \item No SVG may include any rasterized image data or data from external sources.
\end{itemize}
The evaluation system requirements (5-minute generation time limit and 9-hour total generation time limit) will also implicitly influence the design of the model and the training process, although they are more related to the efficiency of the implementation rather than strict output constraints.


\end{document}